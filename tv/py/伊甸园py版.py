import requests
import re
import time
import json
import base64
from urllib.parse import urljoin, unquote
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MobileTVBoxSpider:
    def __init__(self, base_url=None):
        self.base_url = base_url or "https://she.llydy27.xyz/rk.php"
        self.session = requests.Session()
        # æ‰‹æœºç«¯User-Agent
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Linux; Android 10; SM-G981B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Mobile Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        # TVBoxè§£æjarå¼•ç”¨
        self.jar_sniffers = [
            "https://raw.githubusercontent.com/qist/tvbox/master/jar/decoder.jar",
            "https://raw.githubusercontent.com/liu673cn/box/main/m.json",
            "https://fongmi.cachefly.net/0.0.8/jar/decoder.jar"
        ]
        
    def get_page_content(self, url, max_retries=3):
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except Exception as e:
                logger.warning(f"å°è¯• {attempt + 1} å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
                time.sleep(3)
        return None
    
    def extract_categories(self, html_content):
        """æå–æ‰€æœ‰åˆ†ç±»é“¾æ¥å’Œåç§°"""
        categories = []
        
        # å¤šç§åŒ¹é…æ¨¡å¼é€‚åº”ä¸åŒé¡µé¢ç»“æ„
        patterns = [
            r'<li class=".*?"><a href="(/rk\.php/vod/type/id/\d+\.html)"[^>]*>([^<]+)</a></li>',
            r'<a href="(/vod/type/id/\d+\.html)"[^>]*>([^<]+)</a>',
            r'href="(/rk\.php/vod/type/id/\d+\.html)"[^>]*>([^<]+)</a>'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content)
            for url, name in matches:
                if name.strip() and 'ç½‘ç«™é¦–é¡µ' not in name and 'é¦–é¡µ' not in name:
                    full_url = urljoin(self.base_url, url)
                    categories.append({
                        'name': name.strip(),
                        'url': full_url,
                        'id': re.search(r'/id/(\d+)', url).group(1) if re.search(r'/id/(\d+)', url) else ''
                    })
            if categories:
                break
        
        return categories
    
    def get_total_pages(self, html_content, max_pages=50):
        """è·å–æ€»é¡µæ•°ï¼Œé™åˆ¶æœ€å¤§é¡µæ•°"""
        patterns = [
            r'<a class="[^"]*" href="[^"]*/page/(\d+)\.html">(\d+)</a>',
            r'href="[^"]*/page/(\d+)\.html"[^>]*>å°¾é¡µ</a>',
            r'<span[^>]*>(\d+)</span>\s*é¡µ',
            r'å…±\s*(\d+)\s*é¡µ'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content)
            if matches:
                if pattern == patterns[0]:  # ç¬¬ä¸€ç§æ¨¡å¼ç‰¹æ®Šå¤„ç†
                    page_numbers = [int(num) for _, num in matches]
                    total_pages = max(page_numbers) if page_numbers else 1
                else:
                    total_pages = max([int(match) for match in matches if match.isdigit()])
                
                return min(total_pages, max_pages)
        
        return 1
    
    def extract_video_links(self, html_content):
        """ä»é¡µé¢æå–è§†é¢‘è¯¦æƒ…é¡µé“¾æ¥å’Œæ ‡é¢˜"""
        video_data = []
        
        # å¤šç§è§†é¢‘åˆ—è¡¨åŒ¹é…æ¨¡å¼
        patterns = [
            r'<div class="item\s*">\s*<a href="(/rk\.php/vod/detail/id/\d+\.html)"[^>]*>.*?<strong class="title">([^<]+)</strong>',
            r'<a href="(/vod/detail/id/\d+\.html)"[^>]*>.*?<span[^>]*>([^<]+)</span>',
            r'<li[^>]*>\s*<a href="(/vod/detail/id/\d+\.html)"[^>]*>.*?<h3[^>]*>([^<]+)</h3>'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.DOTALL)
            for relative_url, title in matches:
                full_url = urljoin(self.base_url, relative_url)
                video_data.append({
                    'url': full_url,
                    'title': self.clean_title(title.strip())
                })
            if video_data:
                break
        
        return video_data
    
    def clean_title(self, title):
        """æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
        # ç§»é™¤HTMLæ ‡ç­¾
        title = re.sub(r'<[^>]+>', '', title)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œå¸¸è§æ ‡ç‚¹
        title = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\-_ï¼ˆï¼‰()ã€ã€‘\[\]\.]', '', title)
        return title.strip()
    
    def sniff_video_url(self, detail_url, title):
        """å—…æ¢è§†é¢‘æ’­æ”¾åœ°å€ï¼Œæ”¯æŒå¤šç§è§£ææ–¹å¼"""
        html_content = self.get_page_content(detail_url)
        if not html_content:
            return None
        
        # æ–¹æ³•1: ç›´æ¥åŒ¹é…m3u8é“¾æ¥
        m3u8_matches = re.findall(r'https?://[^\s"\'<>]*?\.m3u8(?:\?[^\s"\'<>]*)?', html_content, re.IGNORECASE)
        
        # æ–¹æ³•2: åŒ¹é…base64ç¼–ç çš„m3u8é“¾æ¥
        base64_patterns = [
            r'var\s+[^=]*=\s*["\']([A-Za-z0-9+/=]+40==)["\']',
            r'url\s*:\s*["\']([A-Za-z0-9+/=]{20,})["\']',
            r'video_url\s*=\s*["\']([A-Za-z0-9+/=]{20,})["\']'
        ]
        
        base64_matches = []
        for pattern in base64_patterns:
            matches = re.findall(pattern, html_content)
            for match in matches:
                try:
                    decoded = base64.b64decode(match).decode('utf-8')
                    if '.m3u8' in decoded:
                        base64_matches.append(decoded)
                except:
                    continue
        
        # æ–¹æ³•3: åŒ¹é…JSONæ ¼å¼çš„è§†é¢‘ä¿¡æ¯
        json_patterns = [
            r'var\s+player_\w+\s*=\s*(\{.*?\});',
            r'window\.videoInfo\s*=\s*(\{.*?\});',
            r'var\s+video_data\s*=\s*(\{.*?\});'
        ]
        
        json_matches = []
        for pattern in json_patterns:
            matches = re.findall(pattern, html_content, re.DOTALL)
            for match in matches:
                try:
                    data = json.loads(match)
                    # å°è¯•ä»JSONä¸­æå–urlã€video_urlã€m3u8ç­‰å­—æ®µ
                    for key in ['url', 'video_url', 'm3u8', 'video', 'src']:
                        if key in data and isinstance(data[key], str) and '.m3u8' in data[key]:
                            json_matches.append(data[key])
                except:
                    continue
        
        # æ–¹æ³•4: åŒ¹é…iframeä¸­çš„è§†é¢‘åœ°å€
        iframe_matches = re.findall(r'<iframe[^>]*src="([^"]*)"[^>]*>', html_content, re.IGNORECASE)
        for iframe_url in iframe_matches:
            full_iframe_url = urljoin(detail_url, iframe_url)
            iframe_content = self.get_page_content(full_iframe_url)
            if iframe_content:
                iframe_m3u8 = re.findall(r'https?://[^\s"\'<>]*?\.m3u8', iframe_content, re.IGNORECASE)
                m3u8_matches.extend(iframe_m3u8)
        
        # åˆå¹¶æ‰€æœ‰åŒ¹é…ç»“æœ
        all_matches = m3u8_matches + base64_matches + json_matches
        
        # å»é‡å’Œè¿‡æ»¤
        unique_links = []
        seen_links = set()
        for link in all_matches:
            clean_link = unquote(link.split('"')[0].split("'")[0].split('\\')[0])
            if (clean_link and clean_link not in seen_links and 
                ('.m3u8' in clean_link.lower() or '.mp4' in clean_link.lower())):
                # ç¡®ä¿æ˜¯å®Œæ•´çš„URL
                if clean_link.startswith('http'):
                    unique_links.append(clean_link)
                    seen_links.add(clean_link)
        
        # è¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„é“¾æ¥
        return unique_links[0] if unique_links else None
    
    def generate_tvbox_format(self, title, video_url, category_name):
        """ç”ŸæˆTVBoxå…¼å®¹çš„æ ¼å¼"""
        # TVBoxæ ‡å‡†æ ¼å¼
        return {
            "name": title,
            "url": video_url,
            "type": 0,  # 0è¡¨ç¤ºç›´æ¥æ’­æ”¾
            "playerType": 1,  # 1è¡¨ç¤ºç³»ç»Ÿæ’­æ”¾å™¨
            "headers": {
                "User-Agent": "Mozilla/5.0 (Linux; Android 10; SM-G981B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Mobile Safari/537.36",
                "Referer": self.base_url
            }
        }
    
    def crawl_category_page(self, category_name, page_url, page_num, total_pages, output_file):
        """çˆ¬å–å•ä¸ªåˆ†ç±»é¡µé¢çš„è§†é¢‘ï¼Œå¹¶å®æ—¶ä¿å­˜"""
        logger.info(f"çˆ¬å– {category_name} ç¬¬ {page_num}/{total_pages} é¡µ")
        
        page_content = self.get_page_content(page_url)
        if not page_content:
            logger.warning(f"æ— æ³•è·å– {category_name} ç¬¬ {page_num} é¡µå†…å®¹")
            return 0
        
        # è·å–æœ¬é¡µæ‰€æœ‰è§†é¢‘è¯¦æƒ…é¡µé“¾æ¥å’Œæ ‡é¢˜
        video_data = self.extract_video_links(page_content)
        logger.info(f"{category_name} ç¬¬ {page_num} é¡µ: æ‰¾åˆ° {len(video_data)} ä¸ªè§†é¢‘")
        
        # å¦‚æœæ˜¯ç¬¬ä¸€é¡µï¼Œå†™å…¥åˆ†ç±»æ ‡é¢˜
        if page_num == 1:
            with open(output_file, 'a', encoding='utf-8') as f:
                f.write(f"\n{category_name},#genre#\n")
        
        # å¼‚æ­¥å—…æ¢è§†é¢‘é“¾æ¥
        success_count = 0
        
        with ThreadPoolExecutor(max_workers=3) as executor:  # å‡å°‘çº¿ç¨‹æ•°é¿å…è¢«å°
            future_to_video = {
                executor.submit(self.sniff_video_url, video['url'], video['title']): video 
                for video in video_data
            }
            
            for future in as_completed(future_to_video):
                video = future_to_video[future]
                try:
                    video_url = future.result(timeout=30)  # è®¾ç½®è¶…æ—¶
                    if video_url:
                        # å®æ—¶å†™å…¥æ–‡ä»¶
                        with open(output_file, 'a', encoding='utf-8') as f:
                            f.write(f"{video['title']},{video_url}\n")
                        success_count += 1
                        logger.info(f"æˆåŠŸæå–: {video['title'][:20]}...")
                except Exception as e:
                    logger.error(f"æå–è§†é¢‘é“¾æ¥å¤±è´¥: {str(e)}")
        
        logger.info(f"{category_name} ç¬¬ {page_num} é¡µ: æˆåŠŸæå– {success_count}/{len(video_data)} ä¸ªè§†é¢‘é“¾æ¥")
        return success_count
    
    def crawl_category(self, category, max_pages=50, output_file="mobile_tvbox_videos.txt"):
        """çˆ¬å–å•ä¸ªåˆ†ç±»çš„æ‰€æœ‰é¡µé¢çš„è§†é¢‘é“¾æ¥ï¼Œå¹¶å®æ—¶ä¿å­˜"""
        category_name = category['name']
        base_url = category['url']
        
        logger.info(f"å¼€å§‹çˆ¬å–åˆ†ç±»: {category_name}")
        
        total_success = 0
        
        # è·å–ç¬¬ä¸€é¡µå†…å®¹æ¥ç¡®å®šæ€»é¡µæ•°
        first_page_content = self.get_page_content(base_url)
        if not first_page_content:
            logger.error(f"æ— æ³•è·å–åˆ†ç±»é¦–é¡µ: {category_name}")
            return total_success
        
        total_pages = self.get_total_pages(first_page_content, max_pages)
        logger.info(f"åˆ†ç±» {category_name} å…±æœ‰ {total_pages} é¡µ")
        
        # çˆ¬å–æ‰€æœ‰é¡µé¢
        for page in range(1, total_pages + 1):
            if page == 1:
                page_url = base_url
            else:
                # å¤šç§åˆ†é¡µURLæ ¼å¼æ”¯æŒ
                if '/page/' in base_url:
                    page_url = re.sub(r'/page/\d+', f'/page/{page}', base_url)
                else:
                    page_url = base_url.replace('.html', f'/page/{page}.html')
            
            page_success = self.crawl_category_page(category_name, page_url, page, total_pages, output_file)
            total_success += page_success
            
            # é¡µé¢é—´å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            if page < total_pages:
                time.sleep(2)
        
        logger.info(f"åˆ†ç±» {category_name} å®Œæˆ: æ€»å…±æ‰¾åˆ° {total_success} ä¸ªè§†é¢‘é“¾æ¥")
        return total_success
    
    def crawl_all_categories(self, max_workers=2, max_pages=50, output_file="mobile_tvbox_videos.txt"):
        """çˆ¬å–æ‰€æœ‰åˆ†ç±»çš„æ‰€æœ‰é¡µé¢"""
        # åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# TVBoxæ‰‹æœºç‰ˆè§†é¢‘æº\n")
            f.write("# ç”Ÿæˆæ—¶é—´: " + time.strftime("%Y-%m-%d %H:%M:%S") + "\n")
            f.write("# æ ¼å¼: åˆ†ç±»å,#genre#\n")
            f.write("#       è§†é¢‘æ ‡é¢˜,è§†é¢‘é“¾æ¥\n\n")
        
        # è·å–é¦–é¡µå†…å®¹
        homepage_content = self.get_page_content(self.base_url)
        if not homepage_content:
            logger.error("æ— æ³•è·å–é¦–é¡µå†…å®¹")
            return {}
        
        # æå–æ‰€æœ‰åˆ†ç±»
        categories = self.extract_categories(homepage_content)
        logger.info(f"æ‰¾åˆ° {len(categories)} ä¸ªåˆ†ç±»")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œçˆ¬å–ï¼ˆå‡å°‘çº¿ç¨‹æ•°é¿å…è¢«å°ï¼‰
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_category = {
                executor.submit(self.crawl_category, category, max_pages, output_file): category 
                for category in categories
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            category_results = {}
            for future in as_completed(future_to_category):
                category = future_to_category[future]
                try:
                    success_count = future.result()
                    category_results[category['name']] = success_count
                    logger.info(f"åˆ†ç±» {category['name']} çˆ¬å–å®Œæˆï¼Œå…± {success_count} ä¸ªé“¾æ¥")
                except Exception as e:
                    logger.error(f"çˆ¬å–åˆ†ç±» {category['name']} æ—¶å‡ºé”™: {str(e)}")
                    category_results[category['name']] = 0
        
        return category_results

def main():
    """ä¸»å‡½æ•°"""
    BASE_URL = "https://she.llydy27.xyz/rk.php"
    OUTPUT_FILE = "mobile_tvbox_videos.txt"
    
    spider = MobileTVBoxSpider(BASE_URL)
    
    print("=" * 60)
    print("æ‰‹æœºTVBoxè§†é¢‘å—…æ¢è§£æå™¨")
    print("=" * 60)
    print("ç‰¹ç‚¹:")
    print("  âœ“ æ‰‹æœºç«¯ä¼˜åŒ–")
    print("  âœ“ å¤šè§£ææ–¹å¼æ”¯æŒ")
    print("  âœ“ TVBoxæ ¼å¼å…¼å®¹")
    print("  âœ“ å®æ—¶ä¿å­˜æ•°æ®")
    print("  âœ“ æ™ºèƒ½å»é‡è¿‡æ»¤")
    print("=" * 60)
    
    try:
        # çˆ¬å–æ‰€æœ‰åˆ†ç±»çš„æ‰€æœ‰é¡µé¢ï¼ˆæœ€å¤š50é¡µï¼‰
        results = spider.crawl_all_categories(
            max_workers=2, 
            max_pages=50, 
            output_file=OUTPUT_FILE
        )
        
        # ç»Ÿè®¡æ€»æ•°
        total_links = sum(results.values())
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼æ€»å…±æ‰¾åˆ° {total_links} ä¸ªè§†é¢‘é“¾æ¥")
        
        # æ˜¾ç¤ºå„åˆ†ç±»ç»Ÿè®¡
        print("\nğŸ“Š å„åˆ†ç±»ç»Ÿè®¡:")
        for category_name, count in results.items():
            print(f"  ğŸ“ {category_name}: {count} ä¸ªè§†é¢‘é“¾æ¥")
        
        print(f"\nğŸ’¾ æ–‡ä»¶å·²ä¿å­˜: {OUTPUT_FILE}")
        print("\nğŸ“º TVBoxä½¿ç”¨è¯´æ˜:")
        print("  1. å°†æ–‡ä»¶å¯¼å…¥TVBoxåº”ç”¨")
        print("  2. é€‰æ‹©ç›¸åº”çš„åˆ†ç±»å³å¯æ’­æ”¾")
        print("  3. æ”¯æŒm3u8ã€mp4ç­‰æ ¼å¼")
        print("\nâš ï¸  æ³¨æ„: è¯·éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ï¼Œåˆç†ä½¿ç”¨")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        print(f"ğŸ’¾ å·²çˆ¬å–çš„æ•°æ®å·²ä¿å­˜åˆ°: {OUTPUT_FILE}")
    except Exception as e:
        logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        print(f"ğŸ’¾ éƒ¨åˆ†æ•°æ®å·²ä¿å­˜åˆ°: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()