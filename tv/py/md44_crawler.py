#åœ°å€å‘å¸ƒwww.7000.me
import requests
import re
import time
import os
import json
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

class MD44Crawler:
    def __init__(self):
        self.base_url = "https://www.md44.cc"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://www.md44.cc/',
            'Connection': 'keep-alive',
        })
        self.failed_urls = []
        
    def get_all_categories(self):
        """è·å–å…¨éƒ¨åˆ†ç±»"""
        print("ğŸ¯ æ­£åœ¨è·å–å…¨éƒ¨åˆ†ç±»...")
        categories = []
        
        try:
            response = self.session.get(self.base_url, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æ–¹æ³•1: ä»å¯¼èˆªèœå•è·å–åˆ†ç±»
                nav_links = soup.find_all('a', href=re.compile(r'/(list|category|type)/'))
                for link in nav_links:
                    href = link.get('href', '')
                    name = link.get_text().strip()
                    
                    if (name and len(name) > 1 and 
                        not any(x in name for x in ['é¦–é¡µ', 'Home', 'ä¸»é¡µ', 'ç™»å½•', 'æ³¨å†Œ', 'æœç´¢', 'æ›´å¤š'])):
                        full_url = urljoin(self.base_url, href)
                        categories.append({
                            'name': name,
                            'url': full_url
                        })
                        print(f"ğŸ“‚ æ‰¾åˆ°åˆ†ç±»: {name}")
                
                # æ–¹æ³•2: ä»åˆ†ç±»åŒºå—è·å–
                category_sections = soup.find_all(['div', 'ul', 'nav'], class_=re.compile(r'nav|menu|cate|type', re.I))
                for section in category_sections:
                    links = section.find_all('a', href=True)
                    for link in links:
                        href = link.get('href')
                        name = link.get_text().strip()
                        if (href and name and len(name) > 1 and 
                            re.search(r'/(list|category|type)/', href) and
                            not any(x in name for x in ['é¦–é¡µ', 'Home'])):
                            full_url = urljoin(self.base_url, href)
                            # å»é‡
                            if not any(cat['url'] == full_url for cat in categories):
                                categories.append({
                                    'name': name,
                                    'url': full_url
                                })
                                print(f"ğŸ“‚ æ‰¾åˆ°åˆ†ç±»: {name}")
                
                # å¦‚æœæ²¡æ‰¾åˆ°åˆ†ç±»ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»
                if not categories:
                    default_cats = [
                        {'name': 'æœ€æ–°', 'url': f'{self.base_url}/vod/show/latest.html'},
                        {'name': 'çƒ­é—¨', 'url': f'{self.base_url}/vod/show/hot.html'},
                        {'name': 'æ¨è', 'url': f'{self.base_url}/vod/show/recommend.html'},
                    ]
                    categories.extend(default_cats)
                    print("ğŸ“‚ ä½¿ç”¨é»˜è®¤åˆ†ç±»")
                
                print(f"âœ… å…±æ‰¾åˆ° {len(categories)} ä¸ªåˆ†ç±»")
                return categories
                
        except Exception as e:
            print(f"âŒ è·å–åˆ†ç±»å¤±è´¥: {e}")
        
        return []

    def get_total_pages(self, category_url):
        """è·å–åˆ†ç±»æ€»é¡µæ•°"""
        try:
            response = self.session.get(category_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾åˆ†é¡µä¿¡æ¯
                page_links = soup.find_all('a', href=re.compile(r'page=|list_\d+'))
                page_numbers = []
                
                for link in page_links:
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    
                    # ä»URLä¸­æå–é¡µç 
                    page_match = re.search(r'page=(\d+)', href)
                    if page_match:
                        page_numbers.append(int(page_match.group(1)))
                    
                    # ä»æ–‡æœ¬ä¸­æå–é¡µç 
                    if text.isdigit():
                        page_numbers.append(int(text))
                
                # æŸ¥æ‰¾æœ€åä¸€é¡µ
                last_page_links = soup.find_all('a', string=re.compile(r'æœ«é¡µ|æœ€å|å°¾é¡µ|last', re.I))
                for link in last_page_links:
                    href = link.get('href', '')
                    page_match = re.search(r'page=(\d+)', href)
                    if page_match:
                        return int(page_match.group(1))
                
                if page_numbers:
                    return max(page_numbers)
                    
        except Exception as e:
            print(f"   âŒ è·å–æ€»é¡µæ•°å¤±è´¥: {e}")
        
        return 50  # é»˜è®¤è¿”å›50é¡µ

    def get_videos_from_page(self, category_url, page=1):
        """ä»é¡µé¢è·å–è§†é¢‘åˆ—è¡¨"""
        videos = []
        try:
            # æ„é€ åˆ†é¡µURL
            if '?' in category_url:
                page_url = f"{category_url}&page={page}"
            else:
                page_url = f"{category_url}?page={page}" if '?' not in category_url else f"{category_url}&page={page}"
            
            print(f"   ğŸ“„ è·å–ç¬¬ {page} é¡µ")
            response = self.session.get(page_url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾è§†é¢‘åˆ—è¡¨å®¹å™¨
                video_containers = soup.find_all(['div', 'li'], class_=re.compile(r'video|item|box|card', re.I))
                
                for container in video_containers:
                    # æŸ¥æ‰¾è§†é¢‘é“¾æ¥
                    video_link = container.find('a', href=re.compile(r'/vod/detail/|/video/|/play/'))
                    if video_link:
                        href = video_link.get('href')
                        title = video_link.get('title') or video_link.get_text().strip()
                        
                        if href and title:
                            full_url = urljoin(self.base_url, href)
                            
                            # æŸ¥æ‰¾å›¾ç‰‡
                            img = container.find('img')
                            img_url = img.get('src') if img else ""
                            if img_url and not img_url.startswith('http'):
                                img_url = urljoin(self.base_url, img_url)
                            
                            video_data = {
                                'title': title,
                                'url': full_url,
                                'image': img_url,
                                'page_url': page_url
                            }
                            
                            # å»é‡
                            if not any(v['url'] == full_url for v in videos):
                                videos.append(video_data)
                
                # å¦‚æœBeautifulSoupæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
                if not videos:
                    html = response.text
                    video_patterns = [
                        r'<a[^>]*href="(/vod/detail/[^"]+)"[^>]*title="([^"]+)"',
                        r'<a[^>]*href="(/video/[^"]+)"[^>]*>.*?<img[^>]*src="([^"]*)"[^>]*>.*?<h3[^>]*>([^<]+)</h3>',
                        r'<a[^>]*href="(/play/[^"]+)"[^>]*>([^<]+)</a>',
                    ]
                    
                    for pattern in video_patterns:
                        matches = re.findall(pattern, html, re.DOTALL)
                        for match in matches:
                            if len(match) >= 2:
                                href = match[0]
                                title = match[1] if len(match) == 2 else match[2]
                                
                                if href and title:
                                    full_url = urljoin(self.base_url, href)
                                    videos.append({
                                        'title': title.strip(),
                                        'url': full_url,
                                        'image': '',
                                        'page_url': page_url
                                    })
                
                print(f"   âœ… æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")
                return videos
                
        except Exception as e:
            print(f"   âŒ è·å–ç¬¬ {page} é¡µå¤±è´¥: {e}")
            self.failed_urls.append(f"é¡µé¢ {page_url}: {e}")
        
        return videos

    def get_all_pages_videos(self, category_url):
        """è·å–åˆ†ç±»çš„æ‰€æœ‰é¡µé¢è§†é¢‘"""
        all_videos = []
        
        # å…ˆè·å–æ€»é¡µæ•°
        total_pages = self.get_total_pages(category_url)
        print(f"   ğŸ“– æ€»é¡µæ•°: {total_pages}")
        
        for page in range(1, total_pages + 1):
            videos = self.get_videos_from_page(category_url, page)
            
            if not videos:
                print(f"   â¹ï¸  ç¬¬ {page} é¡µæ²¡æœ‰è§†é¢‘ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤è§†é¢‘
            new_videos = [v for v in videos if not any(av['url'] == v['url'] for av in all_videos)]
            
            if not new_videos and page > 1:
                print(f"   â¹ï¸  ç¬¬ {page} é¡µéƒ½æ˜¯é‡å¤è§†é¢‘ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            all_videos.extend(new_videos)
            
            # å¦‚æœè¿ç»­3é¡µè§†é¢‘æ•°é‡å¾ˆå°‘ï¼Œå¯èƒ½åˆ°è¾¾æœ«å°¾
            if len(videos) < 5 and page > 3:
                print(f"   â¹ï¸  è¿ç»­é¡µé¢è§†é¢‘è¾ƒå°‘ï¼Œå¯èƒ½åˆ°è¾¾æœ«å°¾")
                break
            
            page += 1
            time.sleep(0.3)  # é¡µé—´å»¶è¿Ÿ
        
        print(f"   ğŸ“Š åˆ†ç±»å…±è·å– {len(all_videos)} ä¸ªè§†é¢‘")
        return all_videos

    def extract_play_url(self, html, video_url):
        """ä»è§†é¢‘é¡µé¢æå–æ’­æ”¾åœ°å€"""
        play_url = None
        
        # æ–¹æ³•1: æœç´¢m3u8æ–‡ä»¶
        m3u8_patterns = [
            r'["\'](https?://[^"\']+\.m3u8[^"\']*)["\']',
            r'src\s*:\s*["\'](https?://[^"\']+\.m3u8[^"\']*)["\']',
            r'file\s*:\s*["\'](https?://[^"\']+\.m3u8[^"\']*)["\']',
            r'url\s*:\s*["\'](https?://[^"\']+\.m3u8[^"\']*)["\']',
            r'video_url\s*:\s*["\'](https?://[^"\']+\.m3u8[^"\']*)["\']',
        ]
        
        for pattern in m3u8_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            for match in matches:
                if '.m3u8' in match.lower():
                    play_url = match
                    break
            if play_url:
                break
        
        # æ–¹æ³•2: æœç´¢iframe
        if not play_url:
            iframe_match = re.search(r'<iframe[^>]*src="([^"]+)"', html)
            if iframe_match:
                iframe_src = iframe_match.group(1)
                if iframe_src.startswith('//'):
                    iframe_src = 'https:' + iframe_src
                elif iframe_src.startswith('/'):
                    iframe_src = urljoin(self.base_url, iframe_src)
                play_url = iframe_src
        
        # æ–¹æ³•3: æœç´¢è§†é¢‘æ’­æ”¾å™¨
        if not play_url:
            video_tags = re.findall(r'<video[^>]*src="([^"]+)"', html)
            for video_src in video_tags:
                if video_src and len(video_src) > 10:
                    play_url = video_src
                    break
        
        # æ–¹æ³•4: æœç´¢JavaScriptå˜é‡
        if not play_url:
            js_patterns = [
                r'var\s+url\s*=\s*["\']([^"\']+)["\']',
                r'var\s+video_url\s*=\s*["\']([^"\']+)["\']',
                r'var\s+src\s*=\s*["\']([^"\']+)["\']',
            ]
            for pattern in js_patterns:
                matches = re.findall(pattern, html)
                for match in matches:
                    if any(ext in match for ext in ['.m3u8', '.mp4', '.flv']):
                        play_url = match
                        break
                if play_url:
                    break
        
        # å¤„ç†ç›¸å¯¹URL
        if play_url:
            if play_url.startswith('//'):
                play_url = 'https:' + play_url
            elif play_url.startswith('/'):
                play_url = urljoin(self.base_url, play_url)
        
        return play_url

    def get_video_play_url(self, video_url):
        """è·å–è§†é¢‘æ’­æ”¾åœ°å€"""
        try:
            print(f"    ğŸ¬ è§£æè§†é¢‘é¡µé¢")
            response = self.session.get(video_url, timeout=15)
            
            if response.status_code == 200:
                play_url = self.extract_play_url(response.text, video_url)
                
                if play_url:
                    print(f"    âœ… æ‰¾åˆ°æ’­æ”¾åœ°å€")
                    return play_url
                else:
                    # å°è¯•ä»æ’­æ”¾å™¨é¡µé¢è¿›ä¸€æ­¥è§£æ
                    play_url = self.deep_parse_video(response.text, video_url)
                    if play_url:
                        print(f"    âœ… æ·±åº¦è§£ææ‰¾åˆ°æ’­æ”¾åœ°å€")
                        return play_url
                    else:
                        print(f"    âŒ æœªæ‰¾åˆ°æ’­æ”¾åœ°å€")
                        return None
            else:
                print(f"    âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"    âŒ è§£æè§†é¢‘å¤±è´¥: {e}")
            self.failed_urls.append(f"è§†é¢‘ {video_url}: {e}")
            return None

    def deep_parse_video(self, html, video_url):
        """æ·±åº¦è§£æè§†é¢‘é¡µé¢"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # æŸ¥æ‰¾æ’­æ”¾å™¨iframe
            iframe = soup.find('iframe', src=True)
            if iframe:
                iframe_src = iframe['src']
                if iframe_src.startswith('//'):
                    iframe_src = 'https:' + iframe_src
                elif iframe_src.startswith('/'):
                    iframe_src = urljoin(self.base_url, iframe_src)
                
                # è·å–iframeå†…å®¹
                iframe_response = self.session.get(iframe_src, timeout=10)
                if iframe_response.status_code == 200:
                    return self.extract_play_url(iframe_response.text, iframe_src)
            
            # æŸ¥æ‰¾è§†é¢‘æ’­æ”¾å™¨è„šæœ¬
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string:
                    script_text = script.string
                    m3u8_matches = re.findall(r'https?://[^\s"\']+\.m3u8[^\s"\']*', script_text)
                    for match in m3u8_matches:
                        if len(match) > 20:
                            return match
            
            return None
            
        except Exception as e:
            print(f"    âŒ æ·±åº¦è§£æå¤±è´¥: {e}")
            return None

    def crawl_complete_site(self):
        """çˆ¬å–æ•´ä¸ªç½‘ç«™çš„æ‰€æœ‰å†…å®¹"""
        print("ğŸš€ å¼€å§‹çˆ¬å– MD44.cc å…¨ç«™å†…å®¹")
        print("=" * 60)
        
        start_time = time.time()
        
        # è·å–æ‰€æœ‰åˆ†ç±»
        categories = self.get_all_categories()
        if not categories:
            print("âŒ æ— æ³•è·å–åˆ†ç±»ï¼Œé€€å‡º")
            return
        
        all_data = {}
        total_videos_count = 0
        
        # éå†æ¯ä¸ªåˆ†ç±»
        for i, category in enumerate(categories, 1):
            print(f"\nğŸ¯ [{i}/{len(categories)}] å¤„ç†åˆ†ç±»: {category['name']}")
            
            # è·å–è¯¥åˆ†ç±»çš„æ‰€æœ‰è§†é¢‘
            category_videos = self.get_all_pages_videos(category['url'])
            
            if not category_videos:
                print(f"âš ï¸  åˆ†ç±» {category['name']} æ²¡æœ‰è·å–åˆ°è§†é¢‘")
                continue
            
            # è·å–æ¯ä¸ªè§†é¢‘çš„æ’­æ”¾åœ°å€
            successful_videos = []
            for j, video in enumerate(category_videos, 1):
                print(f"  ğŸ“¹ [{j}/{len(category_videos)}] {video['title'][:30]}...")
                
                play_url = self.get_video_play_url(video['url'])
                
                if play_url:
                    video_data = {
                        'title': video['title'],
                        'play_url': play_url
                    }
                    successful_videos.append(video_data)
                    print(f"  âœ… æˆåŠŸ")
                else:
                    print(f"  âŒ å¤±è´¥")
                
                # å»¶è¿Ÿé¿å…è¢«å°
                time.sleep(0.5)
            
            if successful_videos:
                all_data[category['name']] = successful_videos
                total_videos_count += len(successful_videos)
                print(f"ğŸ‰ åˆ†ç±» {category['name']} å®Œæˆ: {len(successful_videos)} ä¸ªè§†é¢‘")
            else:
                print(f"âš ï¸  åˆ†ç±» {category['name']} æ²¡æœ‰æˆåŠŸè·å–æ’­æ”¾åœ°å€")
        
        # ä¿å­˜ç»“æœ
        if all_data:
            self.save_results(all_data)
            end_time = time.time()
            
            print(f"\nğŸŠ çˆ¬å–å®Œæˆ!")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   â€¢ åˆ†ç±»æ•°é‡: {len(all_data)}")
            print(f"   â€¢ è§†é¢‘æ€»æ•°: {total_videos_count}")
            print(f"   â€¢ è€—æ—¶: {end_time - start_time:.2f} ç§’")
            print(f"   â€¢ å¤±è´¥è¯·æ±‚: {len(self.failed_urls)}")
            
            if self.failed_urls:
                print(f"   â€¢ å¤±è´¥è¯¦æƒ…å·²ä¿å­˜åˆ° failed_urls.txt")
                with open('failed_urls.txt', 'w', encoding='utf-8') as f:
                    for failed in self.failed_urls:
                        f.write(failed + '\n')
        else:
            print("\nâŒ çˆ¬å–å¤±è´¥ï¼Œæ²¡æœ‰è·å–åˆ°ä»»ä½•è§†é¢‘")

    def save_results(self, data):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        filename = "md44_complete_videos.txt"
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ° {filename}...")
        
        with open(filename, 'w', encoding='utf-8') as f:
            for category_name, videos in data.items():
                # å†™å…¥åˆ†ç±»æ ‡é¢˜
                f.write(f"{category_name},#genre#\n")
                print(f"ğŸ“‚ å†™å…¥åˆ†ç±»: {category_name} ({len(videos)}ä¸ªè§†é¢‘)")
                
                # å†™å…¥è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰è§†é¢‘
                for video in videos:
                    # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                    title = re.sub(r'[,#\n\r\t]', ' ', video['title']).strip()
                    play_url = video['play_url']
                    
                    f.write(f"{title},{play_url}\n")
                
                # åˆ†ç±»é—´æ·»åŠ ç©ºè¡Œ
                f.write("\n")
        
        file_size = os.path.getsize(filename)
        print(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ: {filename} ({file_size} å­—èŠ‚)")

def main():
    print("=" * 60)
    print("ğŸ¬ MD44.cc å…¨ç«™è§†é¢‘çˆ¬è™«")
    print("ğŸ“± ä¸“ä¸ºTermuxä¼˜åŒ–ç‰ˆæœ¬")
    print("=" * 60)
    print("âš ï¸  æ³¨æ„: è¯·ç¡®ä¿åœ¨åˆæ³•èŒƒå›´å†…ä½¿ç”¨æœ¬å·¥å…·")
    print("â³ çˆ¬å–å…¨ç«™å†…å®¹éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print("=" * 60)
    
    crawler = MD44Crawler()
    
    try:
        crawler.crawl_complete_site()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"\nğŸ’¥ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()